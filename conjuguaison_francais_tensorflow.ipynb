{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuguaison de verbes en francais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation de donnnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verbe</th>\n",
       "      <th>groupe</th>\n",
       "      <th>personne</th>\n",
       "      <th>verbe_conjugue</th>\n",
       "      <th>temps_verbal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vêtir</td>\n",
       "      <td>troisième groupe</td>\n",
       "      <td>pps</td>\n",
       "      <td>&lt;start&gt; jevêts &lt;end&gt;</td>\n",
       "      <td>indicatif présent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vêtir</td>\n",
       "      <td>troisième groupe</td>\n",
       "      <td>dps</td>\n",
       "      <td>&lt;start&gt; tuvêts &lt;end&gt;</td>\n",
       "      <td>indicatif présent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vêtir</td>\n",
       "      <td>troisième groupe</td>\n",
       "      <td>tps</td>\n",
       "      <td>&lt;start&gt; ilvêt &lt;end&gt;</td>\n",
       "      <td>indicatif présent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vêtir</td>\n",
       "      <td>troisième groupe</td>\n",
       "      <td>ppp</td>\n",
       "      <td>&lt;start&gt; nousvêtons &lt;end&gt;</td>\n",
       "      <td>indicatif présent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vêtir</td>\n",
       "      <td>troisième groupe</td>\n",
       "      <td>dpp</td>\n",
       "      <td>&lt;start&gt; vousvêtez &lt;end&gt;</td>\n",
       "      <td>indicatif présent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187963</th>\n",
       "      <td>planter</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>dps</td>\n",
       "      <td>&lt;start&gt; plantons &lt;end&gt;</td>\n",
       "      <td>impératif présent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187964</th>\n",
       "      <td>planter</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>tps</td>\n",
       "      <td>&lt;start&gt; plantez &lt;end&gt;</td>\n",
       "      <td>impératif présent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187965</th>\n",
       "      <td>planter</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>pps</td>\n",
       "      <td>&lt;start&gt; aie planté &lt;end&gt;</td>\n",
       "      <td>impératif passé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187966</th>\n",
       "      <td>planter</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>dps</td>\n",
       "      <td>&lt;start&gt; ayons planté &lt;end&gt;</td>\n",
       "      <td>impératif passé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187967</th>\n",
       "      <td>planter</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>tps</td>\n",
       "      <td>&lt;start&gt; ayez planté &lt;end&gt;</td>\n",
       "      <td>impératif passé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187968 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          verbe            groupe personne              verbe_conjugue  \\\n",
       "0         vêtir  troisième groupe      pps        <start> jevêts <end>   \n",
       "1         vêtir  troisième groupe      dps        <start> tuvêts <end>   \n",
       "2         vêtir  troisième groupe      tps         <start> ilvêt <end>   \n",
       "3         vêtir  troisième groupe      ppp    <start> nousvêtons <end>   \n",
       "4         vêtir  troisième groupe      dpp     <start> vousvêtez <end>   \n",
       "...         ...               ...      ...                         ...   \n",
       "187963  planter    premier groupe      dps      <start> plantons <end>   \n",
       "187964  planter    premier groupe      tps       <start> plantez <end>   \n",
       "187965  planter    premier groupe      pps    <start> aie planté <end>   \n",
       "187966  planter    premier groupe      dps  <start> ayons planté <end>   \n",
       "187967  planter    premier groupe      tps   <start> ayez planté <end>   \n",
       "\n",
       "             temps_verbal  \n",
       "0       indicatif présent  \n",
       "1       indicatif présent  \n",
       "2       indicatif présent  \n",
       "3       indicatif présent  \n",
       "4       indicatif présent  \n",
       "...                   ...  \n",
       "187963  impératif présent  \n",
       "187964  impératif présent  \n",
       "187965    impératif passé  \n",
       "187966    impératif passé  \n",
       "187967    impératif passé  \n",
       "\n",
       "[187968 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('verbes_conjuges_avec_groupe.csv')\n",
    "df1 = df.copy()\n",
    "df1['temps_verbal'] = df1.pop('mode') + ' ' +df1.pop('temps')\n",
    "df1.rename(columns={'conjugaison': 'verbe_conjugue'}, inplace=True)\n",
    "df1['verbe_conjugue'] = '<start> ' + df1['verbe_conjugue'] + ' <end>'\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionnons les colonnes d'entrée en une seule chaîne pour simplifier la tokenisation\n",
    "df1['input'] = df1[['verbe', 'temps_verbal', 'personne']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automation should concerns data processing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisons les entrées et les sorties\n",
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "tokenizer = Tokenizer(filters=filters)\n",
    "\n",
    "# Assurez que toutes les données sont des chaînes de caractères\n",
    "df1['input'] = df1['input'].astype(str)\n",
    "df1['verbe_conjugue'] = df1['verbe_conjugue'].astype(str)\n",
    "\n",
    "tokenizer.fit_on_texts(pd.concat([df1['input'], df1['verbe_conjugue']]))\n",
    "# tokenizer.fit_on_texts(df1['input'])\n",
    "# tokenizer.fit_on_texts(df1['verbe_conjugue'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verbe</th>\n",
       "      <th>groupe</th>\n",
       "      <th>personne</th>\n",
       "      <th>verbe_conjugue</th>\n",
       "      <th>temps_verbal</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44200</th>\n",
       "      <td>louer</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>dpp</td>\n",
       "      <td>&lt;start&gt; vous louerez &lt;end&gt;</td>\n",
       "      <td>indicatif futur simple</td>\n",
       "      <td>louer indicatif futur simple dpp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134948</th>\n",
       "      <td>annuler</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>tps</td>\n",
       "      <td>&lt;start&gt; qu'il eût annulé &lt;end&gt;</td>\n",
       "      <td>subjonctif plus-que-parfait</td>\n",
       "      <td>annuler subjonctif plus-que-parfait tps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36966</th>\n",
       "      <td>adopter</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>pps</td>\n",
       "      <td>&lt;start&gt; j'ai adopté &lt;end&gt;</td>\n",
       "      <td>indicatif passé composé</td>\n",
       "      <td>adopter indicatif passé composé pps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53965</th>\n",
       "      <td>tracer</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>dps</td>\n",
       "      <td>&lt;start&gt; tu traçais &lt;end&gt;</td>\n",
       "      <td>indicatif imparfait</td>\n",
       "      <td>tracer indicatif imparfait dps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>exempter</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>ppp</td>\n",
       "      <td>&lt;start&gt; nous avons exempté &lt;end&gt;</td>\n",
       "      <td>indicatif passé composé</td>\n",
       "      <td>exempter indicatif passé composé ppp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35628</th>\n",
       "      <td>relater</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>pps</td>\n",
       "      <td>&lt;start&gt; je relatais &lt;end&gt;</td>\n",
       "      <td>indicatif imparfait</td>\n",
       "      <td>relater indicatif imparfait pps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47273</th>\n",
       "      <td>arguer</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>tpp</td>\n",
       "      <td>&lt;start&gt; ils argueront &lt;end&gt;</td>\n",
       "      <td>indicatif futur simple</td>\n",
       "      <td>arguer indicatif futur simple tpp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138279</th>\n",
       "      <td>naître</td>\n",
       "      <td>troisième groupe</td>\n",
       "      <td>ppp</td>\n",
       "      <td>&lt;start&gt; nous naîtrons &lt;end&gt;</td>\n",
       "      <td>indicatif futur simple</td>\n",
       "      <td>naître indicatif futur simple ppp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184788</th>\n",
       "      <td>peinturer</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>pps</td>\n",
       "      <td>&lt;start&gt; j'eusse peinturé &lt;end&gt;</td>\n",
       "      <td>conditionnel passé deuxième forme</td>\n",
       "      <td>peinturer conditionnel passé deuxième forme pps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174918</th>\n",
       "      <td>diffuser</td>\n",
       "      <td>premier groupe</td>\n",
       "      <td>pps</td>\n",
       "      <td>&lt;start&gt; j'ai diffusé &lt;end&gt;</td>\n",
       "      <td>indicatif passé composé</td>\n",
       "      <td>diffuser indicatif passé composé pps</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            verbe            groupe personne  \\\n",
       "44200       louer    premier groupe      dpp   \n",
       "134948    annuler    premier groupe      tps   \n",
       "36966     adopter    premier groupe      pps   \n",
       "53965      tracer    premier groupe      dps   \n",
       "3369     exempter    premier groupe      ppp   \n",
       "...           ...               ...      ...   \n",
       "35628     relater    premier groupe      pps   \n",
       "47273      arguer    premier groupe      tpp   \n",
       "138279     naître  troisième groupe      ppp   \n",
       "184788  peinturer    premier groupe      pps   \n",
       "174918   diffuser    premier groupe      pps   \n",
       "\n",
       "                          verbe_conjugue                       temps_verbal  \\\n",
       "44200         <start> vous louerez <end>             indicatif futur simple   \n",
       "134948    <start> qu'il eût annulé <end>        subjonctif plus-que-parfait   \n",
       "36966          <start> j'ai adopté <end>            indicatif passé composé   \n",
       "53965           <start> tu traçais <end>                indicatif imparfait   \n",
       "3369    <start> nous avons exempté <end>            indicatif passé composé   \n",
       "...                                  ...                                ...   \n",
       "35628          <start> je relatais <end>                indicatif imparfait   \n",
       "47273        <start> ils argueront <end>             indicatif futur simple   \n",
       "138279       <start> nous naîtrons <end>             indicatif futur simple   \n",
       "184788    <start> j'eusse peinturé <end>  conditionnel passé deuxième forme   \n",
       "174918        <start> j'ai diffusé <end>            indicatif passé composé   \n",
       "\n",
       "                                                  input  \n",
       "44200                  louer indicatif futur simple dpp  \n",
       "134948          annuler subjonctif plus-que-parfait tps  \n",
       "36966               adopter indicatif passé composé pps  \n",
       "53965                    tracer indicatif imparfait dps  \n",
       "3369               exempter indicatif passé composé ppp  \n",
       "...                                                 ...  \n",
       "35628                   relater indicatif imparfait pps  \n",
       "47273                 arguer indicatif futur simple tpp  \n",
       "138279                naître indicatif futur simple ppp  \n",
       "184788  peinturer conditionnel passé deuxième forme pps  \n",
       "174918             diffuser indicatif passé composé pps  \n",
       "\n",
       "[20000 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertissons les textes en séquences d'entiers\n",
    "sequence_inputs = tokenizer.texts_to_sequences(df1['input'])\n",
    "sequence_outputs = tokenizer.texts_to_sequences(df1['verbe_conjugue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paddons les séquences pour qu'elles aient toutes la même longueur\n",
    "max_seq_length = max(max(len(seq) for seq in sequence_inputs), max(len(seq) for seq in sequence_outputs))\n",
    "encoder_input_data = pad_sequences(sequence_inputs, maxlen=max_seq_length, padding='post')\n",
    "decoder_input_data = pad_sequences(sequence_outputs, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparons les données de sortie (décalées d'un pas de temps et catégorisées)\n",
    "decoder_output_data = np.zeros_like(decoder_input_data)\n",
    "decoder_output_data[:, 0:-1] = decoder_input_data[:, 1:]\n",
    "decoder_output_data = to_categorical(decoder_output_data, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Séparation initiale des entrées\n",
    "    inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(\n",
    "        np.hstack((encoder_input_data, decoder_input_data)),  # Empilage horizontal pour conserver la correspondance\n",
    "        decoder_output_data,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Extraire encoder_input et decoder_input des ensembles d'entraînement et de test\n",
    "    encoder_input_train = inputs_train[:, :encoder_input_data.shape[1]]  # Première moitié des colonnes\n",
    "    decoder_input_train = inputs_train[:, encoder_input_data.shape[1]:]  # Seconde moitié des colonnes\n",
    "\n",
    "    encoder_input_test = inputs_test[:, :encoder_input_data.shape[1]]\n",
    "    decoder_input_test = inputs_test[:, encoder_input_data.shape[1]:]\n",
    "\n",
    "    #Sauvegarder les outputs\n",
    "    decoder_target_train = outputs_train\n",
    "    decoder_target_test = outputs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "encoder_input_train, encoder_input_test, decoder_input_train, decoder_input_test, decoder_output_train, decoder_output_test = train_test_split(encoder_input_data, decoder_input_data, decoder_output_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele encodeur decodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, Concatenate, TimeDistributed, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactMatch(Metric):\n",
    "    def __init__(self, name='exact_match', **kwargs):\n",
    "        super(ExactMatch, self).__init__(name=name, **kwargs)\n",
    "        self.correct_predictions = self.add_weight(name='cp', initializer='zeros')\n",
    "        self.total_predictions = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        matches = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), y_pred), 'float32')\n",
    "        self.correct_predictions.assign_add(tf.reduce_sum(matches))\n",
    "        # self.total_predictions.assign_add(tf.size(matches))\n",
    "        self.total_predictions.assign_add(tf.cast(tf.size(matches), 'float32'))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_predictions / self.total_predictions\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.correct_predictions.assign(0.)\n",
    "        self.total_predictions.assign(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec une chouche de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    # Utilisation des valeurs définies lors de la préparation des données\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # Ajoutez 1 pour le token 0 qui n'est pas utilisé\n",
    "    max_seq_length = max_seq_length  # Défini lors de la préparation des données\n",
    "\n",
    "    # Hypothétiques dimensions d'embedding et unités LSTM\n",
    "    embedding_dim = 256\n",
    "    lstm_units = 512\n",
    "\n",
    "    # Encodeur\n",
    "    encoder_inputs = Input(shape=(max_seq_length,))\n",
    "    encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
    "    encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Décodeur\n",
    "    decoder_inputs = Input(shape=(max_seq_length,))\n",
    "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
    "    decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "    # Mécanisme d'attention\n",
    "    attention_layer = Attention()\n",
    "    attention_result = attention_layer([decoder_outputs, encoder_outputs])\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
    "\n",
    "    # Pour la génération de la sortie\n",
    "    decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "    # Construction du modèle\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Résumé du modèle\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec deux couches de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)     (None, 8, 256)               3305472   ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 8, 256)               0         ['embedding_8[0][0]']         \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)       [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)              (None, 8, 512)               1574912   ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)     (None, 8, 256)               3305472   ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 8, 512)               0         ['lstm_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 8, 256)               0         ['embedding_9[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_15 (LSTM)              [(None, 8, 512),             2099200   ['dropout_13[0][0]']          \n",
      "                              (None, 512),                                                        \n",
      "                              (None, 512)]                                                        \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)              (None, 8, 512)               1574912   ['dropout_14[0][0]',          \n",
      "                                                                     'lstm_15[0][1]',             \n",
      "                                                                     'lstm_15[0][2]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 8, 512)               0         ['lstm_16[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_17 (LSTM)              (None, 8, 512)               2099200   ['dropout_15[0][0]']          \n",
      "                                                                                                  \n",
      " attention_4 (Attention)     (None, 8, 512)               0         ['lstm_17[0][0]',             \n",
      "                                                                     'lstm_15[0][0]']             \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)  (None, 8, 1024)              0         ['lstm_17[0][0]',             \n",
      "                                                                     'attention_4[0][0]']         \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDi  (None, 8, 12912)             1323480   ['concat_layer[0][0]']        \n",
      " stributed)                                               0                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27193968 (103.74 MB)\n",
      "Trainable params: 27193968 (103.74 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Utilisation des valeurs définies lors de la préparation des données\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Ajoutez 1 pour le token 0 qui n'est pas utilisé\n",
    "max_seq_length = max_seq_length  # Défini lors de la préparation des données\n",
    "\n",
    "# Hypothétiques dimensions d'embedding et unités LSTM\n",
    "embedding_dim = 256\n",
    "lstm_units = 512\n",
    "\n",
    "# Encodeur\n",
    "encoder_inputs = Input(shape=(max_seq_length,))\n",
    "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_dropout = Dropout(0.5)(encoder_embedding)  # Appliquez le Dropout à l'embedding\n",
    "encoder_lstm1 = LSTM(lstm_units, return_sequences=True, return_state=False)(encoder_dropout)\n",
    "encoder_dropout1 = Dropout(0.5)(encoder_lstm1)  # Appliquez le Dropout après la première couche LSTM\n",
    "encoder_lstm2 = LSTM(lstm_units, return_sequences=True, return_state=True)(encoder_dropout1)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm2 \n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Décodeur\n",
    "decoder_inputs = Input(shape=(max_seq_length,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_dropout = Dropout(0.5)(decoder_embedding)  # Appliquez le Dropout à l'embedding\n",
    "decoder_lstm1 = LSTM(lstm_units, return_sequences=True, return_state=False)(decoder_dropout, initial_state=encoder_states)\n",
    "decoder_dropout1 = Dropout(0.5)(decoder_lstm1)  # Appliquez le Dropout après la première couche LSTM\n",
    "decoder_lstm2 = LSTM(lstm_units, return_sequences=True, return_state=False)(decoder_dropout1)  # Pas besoin d'état initial ici puisque c'est déjà passé à decoder_lstm1\n",
    "decoder_outputs = decoder_lstm2\n",
    "\n",
    "\n",
    "# Mécanisme d'attention\n",
    "attention_layer = Attention()\n",
    "attention_result = attention_layer([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
    "\n",
    "\n",
    "# Pour la génération de la sortie\n",
    "decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Construction du modèle\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement evaluation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_validation_graph(history):\n",
    "    # Accéder à l'historique de la perte\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Accéder à l'historique de la précision\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    # Nombre d'époques\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    # Tracé de la perte\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='Perte d\\'entraînement')\n",
    "    plt.plot(epochs, val_loss, label='Perte de validation')\n",
    "    plt.title('Perte d\\'entraînement et de validation')\n",
    "    plt.xlabel('Époques')\n",
    "    plt.ylabel('Perte')\n",
    "    plt.legend()\n",
    "\n",
    "    # Tracé de la précision\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # plt.plot(epochs, accuracy, 'bo', label='Précision d\\'entraînement')\n",
    "    # plt.plot(epochs, val_accuracy, 'b', label='Précision de validation')\n",
    "    plt.plot(epochs, accuracy, label='Précision d\\'entraînement')\n",
    "    plt.plot(epochs, val_accuracy, label='Précision de validation')\n",
    "    plt.title('Précision d\\'entraînement et de validation')\n",
    "    plt.xlabel('Époques')\n",
    "    plt.ylabel('Précision')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_conjugaison(verbe:str, temps:str, personne:str, model):\n",
    "    input_text = [' '.join([verbe, temps, personne])]\n",
    "    print(input_text)\n",
    "    \n",
    "    # Tokenisation de l'entrée\n",
    "    input_seq = tokenizer.texts_to_sequences(input_text)\n",
    "\n",
    "    # Padding de la séquence pour qu'elle corresponde à la longueur attendue par le modèle\n",
    "    input_seq_padded = pad_sequences(input_seq, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    #Initialisation de la sequnce d'entree du decodeur avec le token de debut\n",
    "    start_token = tokenizer.word_index['<start>']    \n",
    "    end_token = tokenizer.word_index['<end>']\n",
    "    \n",
    "    \n",
    "    predicted_seq = []\n",
    "    for _ in range(max_seq_length):\n",
    "        old_pred = [start_token]\n",
    "        for token in predicted_seq:\n",
    "            if token not in [0, 2]:\n",
    "                old_pred.append(token)\n",
    "        # print('old', old_pred)\n",
    "        decoder_input_seq = np.zeros((1, max_seq_length))\n",
    "        for index, token in  enumerate(old_pred):\n",
    "            if token == 2 and token != 0:\n",
    "                continue\n",
    "            decoder_input_seq[0, index] = token  # Première position avec le token de debut\n",
    "        # Prédiction de la séquence de sortie\n",
    "        # print(decoder_input_seq)\n",
    "        prediction = model.predict([input_seq_padded, decoder_input_seq])#np.zeros((1, max_seq_length))  # La deuxième partie est le début du décodeur\n",
    "        predicted_seq = np.argmax(prediction, axis=-1)[0]  # Prendre l'indice avec la probabilité la plus élevée\n",
    "        # print('predict:', predicted_seq)\n",
    "    \n",
    "    # Conversion de la séquence d'entiers en mots\n",
    "    predicted_text = [tokenizer.index_word[idx] for idx in predicted_seq if (idx > 0) and (idx != start_token) and (idx != end_token)]\n",
    "    \n",
    "\n",
    "    # Assemblage du texte prédit\n",
    "    predicted_conjugation = ' '.join(predicted_text).replace('end', '').strip()\n",
    "    return predicted_conjugation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement\n",
    "\n",
    "Automation can be done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 11:11:16.817917: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"109\" frequency: 2700 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 3145728 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 2.4406 - exact_match: 0.6583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erictamno/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:2620: UserWarning: Metric ExactMatch implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n",
      "2024-03-14 11:22:57.468682: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"109\" frequency: 2700 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 3145728 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 772s 4s/step - loss: 2.4406 - exact_match: 0.6583 - val_loss: 1.7171 - val_exact_match: 0.7143\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 522s 3s/step - loss: 1.5966 - exact_match: 0.7240 - val_loss: 1.5842 - val_exact_match: 0.7516\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 462s 2s/step - loss: 1.3748 - exact_match: 0.7680 - val_loss: 1.4111 - val_exact_match: 0.8040\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 1102s 6s/step - loss: 1.1623 - exact_match: 0.8192 - val_loss: 1.3280 - val_exact_match: 0.8440\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 570s 3s/step - loss: 1.0261 - exact_match: 0.8526 - val_loss: 1.2976 - val_exact_match: 0.8645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f86cd188be0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[ExactMatch()])\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model.fit([encoder_input_train, decoder_input_train], decoder_output_train,\n",
    "          batch_size=64,\n",
    "          epochs=15,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_graph(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 12:09:16.213645: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"109\" frequency: 2700 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 3145728 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 89s 634ms/step - loss: 1.3059 - exact_match: 0.8635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3059414625167847, 0.8634687662124634]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Évaluation du modèle sur les données de test\n",
    "model.evaluate([encoder_input_test, decoder_input_test], decoder_output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['souper indicatif présent ppp']\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nous nous me me skie'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparation_prediction('adopter', 'indicatif passé composé', 'pps', model)\n",
    "prediction_conjugaison('souper', 'indicatif présent', 'ppp', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erictamno/anaconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('model_conjugaison.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
