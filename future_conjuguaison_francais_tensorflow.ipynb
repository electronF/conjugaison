{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuguaison de verbes en francais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation de donnnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 04:20:08.579501: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de structure de votre DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'verbe_infinitif': ['prendre', 'prendre', ...],\n",
    "    'temps_verbal': ['présent de l\\'indicatif', 'présent du subjonctif', ...],\n",
    "    'personne': ['première singulier', 'première singulier', ...],\n",
    "    'verbe_conjugue': ['je prends', 'que je prenne', ...]\n",
    "})\n",
    "\n",
    "# Supposons df est votre DataFrame après avoir chargé et préparé R\n",
    "\n",
    "# Séparation en ensembles d'entraînement et de test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Tokenization et conversion en séquences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pd.concat([train_df['verbe_infinitif'], train_df['temps_verbal'], train_df['personne'], train_df['verbe_conjugue']]))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Préparation des séquences d'entrée et de sortie\n",
    "encoder_input_data = tokenizer.texts_to_sequences(train_df[['verbe_infinitif', 'temps_verbal', 'personne']].apply(lambda x: ' '.join(x), axis=1))\n",
    "decoder_input_data = tokenizer.texts_to_sequences(train_df['verbe_conjugue'])\n",
    "decoder_target_data = pad_sequences(decoder_input_data, maxlen=None, padding='post', truncating='post')\n",
    "\n",
    "encoder_input_data = pad_sequences(encoder_input_data, maxlen=max(encoder_input_data.keymap(len)), padding='post')\n",
    "decoder_input_data = pad_sequences(decoder_input_data, maxlen=max(decoder_input_data.keymap(len)) - 1, padding='post')  # -1 pour shift\n",
    "decoder_target_data = to_categorical(decoder_target_data, num_classes=vocab_size)\n",
    "\n",
    "# Note: Vous devez également préparer les données de test de manière similaire pour l'évaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Chargement de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('liste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulons le chargement de votre DataFrame R\n",
    "# Pour cet exemple, nous utilisons des listes simplifiées\n",
    "data = {\n",
    "    'verbe_infinitif': ['prendre', 'prendre', 'prendre'],\n",
    "    'temps_verbal': ['présent de l\\'indicatif', 'présent du subjonctif', 'passé simple'],\n",
    "    'personne': ['première singulier', 'première singulier', 'première pluriel'],\n",
    "    'verbe_conjugue': ['je prends', 'que je prenne', 'nous prîmes']\n",
    "}\n",
    "\n",
    "# Convertissons notre structure de données en DataFrame pour une manipulation facile\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fusionnons les colonnes d'entrée en une seule chaîne pour simplifier la tokenisation\n",
    "df['input'] = df[['verbe_infinitif', 'temps_verbal', 'personne']].agg(' '.join, axis=1)\n",
    "\n",
    "# Tokenisons les entrées et les sorties\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pd.concat([df['input'], df['verbe_conjugue']]))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convertissons les textes en séquences d'entiers\n",
    "sequence_inputs = tokenizer.texts_to_sequences(df['input'])\n",
    "sequence_outputs = tokenizer.texts_to_sequences(df['verbe_conjugue'])\n",
    "\n",
    "# Paddons les séquences pour qu'elles aient toutes la même longueur\n",
    "max_seq_length = max(max(len(seq) for seq in sequence_inputs), max(len(seq) for seq in sequence_outputs))\n",
    "encoder_input_data = pad_sequences(sequence_inputs, maxlen=max_seq_length, padding='post')\n",
    "decoder_input_data = pad_sequences(sequence_outputs, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Préparons les données de sortie (décalées d'un pas de temps et catégorisées)\n",
    "decoder_output_data = np.zeros_like(decoder_input_data)\n",
    "decoder_output_data[:, 0:-1] = decoder_input_data[:, 1:]\n",
    "decoder_output_data = to_categorical(decoder_output_data, num_classes=vocab_size)\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "encoder_input_train, encoder_input_test, decoder_input_train, decoder_input_test, decoder_output_train, decoder_output_test = train_test_split(encoder_input_data, decoder_input_data, decoder_output_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele encodeur decodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 12:37:36.605171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate, TimeDistributed, Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactMatch(Metric):\n",
    "    def __init__(self, name='exact_match', **kwargs):\n",
    "        super(ExactMatch, self).__init__(name=name, **kwargs)\n",
    "        self.correct_predictions = self.add_weight(name='cp', initializer='zeros')\n",
    "        self.total_predictions = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        matches = tf.cast(tf.equal(tf.argmax(y_true, axis=-1), y_pred), 'float32')\n",
    "        self.correct_predictions.assign_add(tf.reduce_sum(matches))\n",
    "        self.total_predictions.assign_add(tf.size(matches))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_predictions / self.total_predictions\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.correct_predictions.assign(0.)\n",
    "        self.total_predictions.assign(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans mecanisme d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des dimensions\n",
    "embedding_dim = 256\n",
    "lstm_units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Assurez-vous que tokenizer a été défini lors de la préparation des données\n",
    "\n",
    "# Encodeur\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Décodeur\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Modèle\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle\n",
    "model.fit([encoder_input_train, decoder_input_train], decoder_output_train,\n",
    "          batch_size=64,\n",
    "          epochs=100,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du modèle sur les données de test\n",
    "model.evaluate([encoder_input_test, decoder_input_test], decoder_output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec mecanisme d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation des valeurs définies lors de la préparation des données\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Ajoutez 1 pour le token 0 qui n'est pas utilisé\n",
    "max_seq_length = max_seq_length  # Défini lors de la préparation des données\n",
    "\n",
    "# Hypothétiques dimensions d'embedding et unités LSTM\n",
    "embedding_dim = 256\n",
    "lstm_units = 512\n",
    "\n",
    "# Encodeur\n",
    "encoder_inputs = Input(shape=(max_seq_length,))\n",
    "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Décodeur\n",
    "decoder_inputs = Input(shape=(max_seq_length,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Mécanisme d'attention\n",
    "attention_layer = Attention()\n",
    "attention_result = attention_layer([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_result])\n",
    "\n",
    "# Pour la génération de la sortie\n",
    "decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Construction du modèle\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Résumé du modèle\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit([encoder_input_train, decoder_input_train], decoder_output_train,\n",
    "          batch_size=64,\n",
    "          epochs=100,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du modèle sur les données de test\n",
    "model.evaluate([encoder_input_test, decoder_input_test], decoder_output_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
